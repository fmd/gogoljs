{
  "name": "html-tokenize",
  "version": "1.2.5",
  "description": "transform stream to tokenize html",
  "main": "index.js",
  "bin": {
    "html-tokenize": "bin/cmd.js"
  },
  "dependencies": {
    "inherits": "~2.0.1",
    "readable-stream": "~1.0.27-1",
    "minimist": "~0.0.8",
    "through2": "~0.4.1"
  },
  "devDependencies": {
    "tape": "~2.12.1"
  },
  "scripts": {
    "test": "tape test/*.js"
  },
  "repository": {
    "type": "git",
    "url": "git://github.com/substack/html-tokenize.git"
  },
  "homepage": "https://github.com/substack/html-tokenize",
  "keywords": [
    "html",
    "parser",
    "tokenize",
    "stream"
  ],
  "author": {
    "name": "James Halliday",
    "email": "mail@substack.net",
    "url": "http://substack.net"
  },
  "license": "MIT",
  "readme": "# html-tokenize\n\ntransform stream to tokenize html\n\n[![build status](https://secure.travis-ci.org/substack/html-tokenize.png)](http://travis-ci.org/substack/html-tokenize)\n\n# example\n\n``` js\nvar fs = require('fs');\nvar tokenize = require('html-tokenize');\nvar through = require('through2');\n\nfs.createReadStream(__dirname + '/table.html')\n    .pipe(tokenize())\n    .pipe(through.obj(function (row, enc, next) {\n        row[1] = row[1].toString();\n        console.log(row);\n        next();\n    }))\n;\n```\n\nthis html:\n\n``` html\n<table>\n  <tbody>blah blah blah</tbody>\n  <tr><td>there</td></tr>\n  <tr><td>it</td></tr>\n  <tr><td>is</td></tr>\n</table>\n```\n\ngenerates this output:\n\n```\n[ 'open', '<table>' ]\n[ 'text', '\\n  ' ]\n[ 'open', '<tbody>' ]\n[ 'text', 'blah blah blah' ]\n[ 'close', '</tbody>' ]\n[ 'text', '\\n  ' ]\n[ 'open', '<tr>' ]\n[ 'open', '<td>' ]\n[ 'text', 'there' ]\n[ 'close', '</td>' ]\n[ 'close', '</tr>' ]\n[ 'text', '\\n  ' ]\n[ 'open', '<tr>' ]\n[ 'open', '<td>' ]\n[ 'text', 'it' ]\n[ 'close', '</td>' ]\n[ 'close', '</tr>' ]\n[ 'text', '\\n  ' ]\n[ 'open', '<tr>' ]\n[ 'open', '<td>' ]\n[ 'text', 'is' ]\n[ 'close', '</td>' ]\n[ 'close', '</tr>' ]\n[ 'text', '\\n' ]\n[ 'close', '</table>' ]\n[ 'text', '\\n' ]\n```\n\n# methods\n\n``` js\nvar tokenize = require('html-tokenize');\n```\n\n## var t = tokenize()\n\nReturn a tokenize transform stream `t` that takes html input and produces rows\nof output. The output rows are of the form:\n\n* `[ name, buffer ]`\n\nThe input stream maps completely onto the buffers from the object stream.\n\nThe types of names are:\n\n* open\n* close\n* text\n\ncdata, comments, and scripts all use `'open'` with their contents appearing in\nsubsequent `'text'` rows.\n\n# usage\n\nThere is an html-tokenize command too.\n\n```\nusage: html-tokenize {FILE}\n\n  Tokenize FILE into newline-separated json arrays for each tag.\n  If FILE is not specified, use stdin.\n\n```\n\n# install\n\nWith [npm](https://npmjs.org), to get the library do:\n\n```\nnpm install html-tokenize\n```\n\nor to get the command do:\n\n```\nnpm install -g html-tokenize\n```\n\n# license\n\nMIT\n",
  "readmeFilename": "readme.markdown",
  "bugs": {
    "url": "https://github.com/substack/html-tokenize/issues"
  },
  "_id": "html-tokenize@1.2.5",
  "_shasum": "7e5ba99ecb51ef906ec9a7fcdee6ca3267c7897e",
  "_from": "html-tokenize@^1.1.1",
  "_resolved": "https://registry.npmjs.org/html-tokenize/-/html-tokenize-1.2.5.tgz"
}
